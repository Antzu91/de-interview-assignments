{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can install required dependencies this way also\n",
    "%pip install duckdb polars pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: SQL\n",
    "\n",
    "1. Run the cell below to load data from the [bank_failures.parquet](data/bank_failures.parquet) file into a DuckDBPyRelation and inspect the schema.\n",
    "2. In the following cell, calculate the number of **Banks** in the **State** of New York that have failed after 2010-01-01 based on **Date**.  \n",
    "The result only needs to show the number of banks.\n",
    "\n",
    "Look up [DuckDb Python API](https://duckdb.org/docs/api/python/overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "bank_failures_data = duckdb.read_parquet(\"data/bank_failures.parquet\")\n",
    "bank_failures_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"\"\"\n",
    "Write your code here!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: SQL\n",
    "\n",
    "2. In the following cell, calculate the ordinal number of each bank failure per State based on the chronological order of failure.  \n",
    "The result should show the Bank, city, State and the order in which it failed compared to its peers.  \n",
    "Sort the results according to the State, Bank and Date.\n",
    " Hint: Window functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"\"\"\n",
    "Write your code here!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Python (Polars or Pandas)\n",
    "\n",
    "1. Run the cell below to load data from the [yellow_tripdata_2024-01.parquet](data/yellow_tripdata_2024-01.parquet) file into a DataFrame and inspect the schema. You can use either Polars or Pandas  \n",
    "so comment out the one you don't need.\n",
    "2. Filter all trips that with a **trip_distance** greater than 5 miles, and **tip_amount** of 0.\n",
    "3. Write the results to a csv file `temp/no_tip.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pa\n",
    "\n",
    "# polars\n",
    "taxi_df = pl.read_parquet(\"data/yellow_tripdata_2024-01.parquet\")\n",
    "\n",
    "# pandas + pyarrow to load parquet file\n",
    "pa_table = pa.read_table('yellow_tripdata_2024-01.parquet') \n",
    "taxi_df = pa_table.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Python (Polars or Pandas)\n",
    "\n",
    "1. Using the same taxi tripdata from the previous task union it with the data from in [yellow_tripdata_2024-02.parquet](data/yellow_tripdata_2024-02.parquet)\n",
    "2. Group by **tpep_pickup_datetime** but only based on **YYYY-MM-DD**. Count the number of passengers per group, the sum of all payments, the average congestion surcharge.\n",
    "3. Write the results to a parquet file at `temp/daily_trip_stats.parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
